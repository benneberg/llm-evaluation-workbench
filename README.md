# llm-evaluation-workbench
A general-purpose LLM evaluation workspace
This repository contains a lightweight, browser-based workspace for designing, running, and documenting comparative evaluations of large language models.

It is intended for exploratory analysis, research notes, and prompt-driven experiments rather than authoritative benchmarking.
